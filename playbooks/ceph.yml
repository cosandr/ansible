#!/usr/bin/env -S ansible-playbook
---

- name: Ceph host preparation
  hosts: ceph
  gather_facts: true
  gather_subset: ["min"]
  tags: ["system"]
  tasks:
    - name: Download Ceph GPG key
      ansible.builtin.get_url:
        url: https://eu.ceph.com/keys/release.gpg
        dest: /etc/apt/keyrings/ceph-keyring.gpg
        owner: root
        group: root
        mode: "0644"
      diff: false
      tags: ["apt"]

    - name: Add Ceph repo
      ansible.builtin.apt_repository:
        repo: "deb [arch=amd64 signed-by=/etc/apt/keyrings/ceph-keyring.gpg] https://eu.ceph.com/debian-{{ ceph_version }} {{ ansible_distribution_release | lower }} main"
        state: present
        filename: ceph
      tags: ["apt"]

    - name: Prefer repo packages
      ansible.builtin.copy:
        content: |
          Package: *
          Pin: origin ceph.com
          Pin-Priority: 900
        dest: "/etc/apt/preferences.d/99ceph"
        owner: root
        group: root
        mode: "0644"
      tags: ["apt"]

    # Might not work, dpkg-divert should work if this doesn't
    - name: Prevent ceph-common logrotate file from being installed
      ansible.builtin.copy:
        content: |
          path-exclude /etc/logrotate.d/ceph-common
        dest: /etc/dpkg/dpkg.cfg.d/99-no-ceph-common
        owner: root
        group: root
        mode: "0644"
      tags: ["apt"]

    - name: Install Ceph packages
      ansible.builtin.apt:
        name:
          - cephadm
          - ceph-common
        state: present
      tags: ["install"]

- name: Configure Ceph
  hosts: ceph
  gather_facts: false
  run_once: true
  tasks:
    - name: Check if cluster exists
      ansible.builtin.stat:
        path: "/etc/ceph/ceph.client.admin.keyring"
      register: __ceph_exists
      tags: ["bootstrap"]

    - name: Bootstrap cluster
      ceph.automation.cephadm_bootstrap:
        mon_ip: "{{ host_ips.san }}"
        skip_monitoring_stack: true
        log_to_file: true
      when: not __ceph_exists.stat.exists
      notify: Create dashboard ssl cert
      tags: ["bootstrap"]

    - name: Set config values
      ceph.automation.ceph_config:
        action: set
        who: "{{ item.who | default('global') }}"
        option: "{{ item.option }}"
        value: "{{ item.value }}"
      loop: "{{ ceph_config }}"
      tags: ["config"]

    - name: Enable Prometheus
      ceph.automation.ceph_mgr_module:
        name: prometheus
        state: enable
      tags: ["mgr", "prometheus"]

    - name: Get alertmanager API host
      check_mode: false
      changed_when: false
      ansible.builtin.command:
        cmd: ceph dashboard get-alertmanager-api-host
      register: __alertmanager_host
      tags: ["prometheus"]

    - name: Set alertmanager API host  # noqa no-changed-when
      ansible.builtin.command:
        cmd: "ceph dashboard set-alertmanager-api-host {{ ceph_alertmanager_api_host }}"
      when: __alertmanager_host.stdout != ceph_alertmanager_api_host
      tags: ["prometheus"]

    - name: Get prometheus API host
      check_mode: false
      changed_when: false
      ansible.builtin.command:
        cmd: ceph dashboard get-prometheus-api-host
      register: __prometheus_host
      tags: ["prometheus"]

    - name: Set prometheus API host  # noqa no-changed-when
      ansible.builtin.command:
        cmd: "ceph dashboard set-prometheus-api-host {{ ceph_prometheus_api_host }}"
      when: __prometheus_host.stdout != ceph_prometheus_api_host
      tags: ["prometheus"]

    - name: Read Ceph SSH key
      ansible.builtin.slurp:
        path: /etc/ceph/ceph.pub
      register: __ceph_ssh
      tags: ["ssh"]

    - name: Ensure Ceph SSH key exists for root
      run_once: false
      ansible.posix.authorized_key:
        user: root
        key: "{{ __ceph_ssh.content | b64decode }}"
        state: present
      tags: ["ssh"]

    - name: Add hosts
      # Need to run on same host as bootstrap did
      # Error initializing cluster client: ObjectNotFound('RADOS object not found (error calling conf_read_file)')
      # MODULE FAILURE: No start of json char found
      delegate_to: "{{ ansible_play_batch | first }}"
      run_once: false
      throttle: 1
      ceph.automation.ceph_orch_host:
        name: "{{ inventory_hostname }}"
        address: "{{ host_ips.san }}"
        set_admin_label: true
      tags: ["host"]

    - name: Wipe disk
      run_once: false
      tags: ["zap"]
      when: "'zap' in ansible_run_tags"
      block:
        - name: Wait for confirmation
          ansible.builtin.pause:
            prompt: "Are you sure?"

        - name: Wipe SSD  # noqa no-changed-when
          ansible.builtin.shell:
            cmd: |
              set -e
              if ! cephadm shell ceph-volume lvm zap "$disk"; then
                wipefs --all "$disk"
                /usr/bin/dd if=/dev/zero of="$disk" bs=1M count=10 conv=fsync
              fi
          environment:
            disk: "{{ item }}"
          loop:
            - /dev/cephssdvg01/osd
          tags: ["ssd"]
          when: "'ssd' in ansible_run_tags"

        - name: Wipe HDD  # noqa no-changed-when
          ansible.builtin.shell:
            cmd: |
              set -e
              if ! cephadm shell ceph-volume lvm zap "$disk"; then
                wipefs --all "$disk"
                /usr/bin/dd if=/dev/zero of="$disk" bs=1M count=10 conv=fsync
              fi
          environment:
            disk: "{{ item }}"
          tags: ["hdd"]
          loop:
            - /dev/cephtankvg01/osd
            - /dev/vg01/db01
          when: "'hdd' in ansible_run_tags"

    - name: Get host OSDs
      run_once: false
      check_mode: false
      changed_when: false
      ansible.builtin.command:
        cmd: "ceph device ls-by-host {{ inventory_hostname }} -f json"
      # Retry a bit in case host was just added
      retries: 5
      delay: 10
      register: __osd_info
      tags: ["osd"]

    - name: Set OSD info fact
      run_once: false
      ansible.builtin.set_fact:
        osd_info: "{{ __osd_info.stdout | from_json }}"
      tags: ["osd"]

    - name: Create OSDs
      run_once: false
      throttle: 1
      ansible.builtin.shell:
        cmd: |
          set -e
          {% if osd_info | selectattr('devid', 'search', '^SAMSUNG_') | length == 0 %}
          ceph orch daemon add osd {{ inventory_hostname }}:/dev/cephssdvg01/osd
          {% endif %}
          {% if osd_info | selectattr('devid', 'search', '^ST18000NM003D-') | length == 0 %}
          ceph orch daemon add osd {{ inventory_hostname }}:data_devices=/dev/cephtankvg01/osd,db_devices=/dev/vg01/db01
          {% endif %}
          exit 0
      register: output
      changed_when: "'Created osd' in output.stdout"
      tags: ["osd"]

    - name: Apply MDS
      ceph.automation.ceph_orch_apply:
        spec: |
          service_type: mds
          service_id: cephfs
          placement:
            count: 3
      tags: ["mds"]

    - name: Create replicated CRUSH rules
      ceph.automation.ceph_crush_rule:
        name: "replicated_host_{{ item }}"
        state: present
        rule_type: replicated
        device_class: "{{ item }}"
        bucket_root: default
        bucket_type: host
      loop:
        - ssd
        - hdd
      tags: ["crush"]

    - name: Create pools
      ceph.automation.ceph_pool:
        name: "{{ item.name }}"
        state: "{{ item.state | default('present') }}"
        application: "{{ item.application }}"
        pool_type: "{{ item.pool_type }}"
        rule_name: "{{ item.rule_name | default(omit) }}"
        erasure_profile: "{{ item.erasure_profile | default(omit) }}"
      loop: "{{ ceph_pools }}"
      tags: ["pool", "create"]

    - name: Get pool info
      check_mode: false
      changed_when: false
      ansible.builtin.command:
        cmd: ceph osd pool ls detail -f json
      register: __pool_info
      tags: ["pool"]

    - name: Set pool info fact
      ansible.builtin.set_fact:
        pool_info: "{{ __pool_info.stdout | from_json }}"
      tags: ["pool"]

    # TODO: Cannot unset/disable ZSTD or EC overwrites
    - name: Set facts for updating pool options
      ansible.builtin.set_fact:
        pool_set_options: >-
          {%- set tmp = [] -%}
          {%- set zstd_compressed_pools = ceph_pools | selectattr('zstd_compression', 'defined') | selectattr('zstd_compression', 'eq', true) | map(attribute='name') -%}
          {%- set ec_allow_override_pools = ceph_pools | selectattr('ec_overwrites', 'defined') | selectattr('ec_overwrites', 'eq', true) | map(attribute='name') -%}
          {%- for pool in pool_info if pool.pool_name in zstd_compressed_pools -%}
          {%- if pool.options.get("compression_algorithm", None) != "zstd" -%}
          {{- tmp.append({"name": pool.pool_name, "option": "compression_algorithm", "value": "zstd"}) -}}
          {%- endif -%}
          {%- if pool.options.get("compression_mode", None) != "aggressive" -%}
          {{- tmp.append({"name": pool.pool_name, "option": "compression_mode", "value": "aggressive"}) -}}
          {%- endif -%}
          {%- endfor -%}
          {%- for pool in pool_info if pool.pool_name in ec_allow_override_pools -%}
          {%- if "ec_overwrites" not in pool.flags_names -%}
          {{- tmp.append({"name": pool.pool_name, "option": "allow_ec_overwrites", "value": "true"}) -}}
          {%- endif -%}
          {%- endfor -%}
          {{ tmp }}
      tags: ["pool"]

    - name: Set Ceph pool options  # noqa no-changed-when
      ansible.builtin.command:
        cmd: "ceph osd pool set {{ item.name }} {{ item.option }} {{ item.value }}"
      loop: "{{ pool_set_options }}"
      tags: ["pool"]

    - name: Create SSD CephFS
      ceph.automation.ceph_fs:
        name: cephfs
        state: present
        data: cephfs.ssd.data
        metadata: cephfs.ssd.meta
      tags: ["cephfs"]

    - name: Create HDD CephFS
      ceph.automation.ceph_fs:
        name: tank
        state: present
        data: cephfs.hdd.data
        metadata: cephfs.hdd.meta
      tags: ["cephfs"]

    - name: Enable snapshot module
      ceph.automation.ceph_mgr_module:
        name: snap_schedule
        state: enable
      tags: ["cephfs", "snapshot"]

    - name: Configure snapshots
      andrei.utils.ceph_fs_snap_schedule:
        fs: "{{ item.fs }}"
        state: "{{ item.state | default('present') }}"
        schedule: "{{ item.schedule | default(omit) }}"
        path: "{{ item.path }}"
        retention: "{{ item.retention }}"
      loop: "{{ cephfs_snap_schedules }}"
      tags: ["cephfs", "snapshot"]

    - name: Deploy RGW service
      ceph.automation.ceph_orch_apply:
        spec: "{{ lookup('template', repo_base_dir + '/files/ceph/rgw-main.yml.j2') }}"
      tags: ["rgw"]

    - name: Get RGW zonegroup placement info
      check_mode: false
      changed_when: false
      ansible.builtin.command:
        cmd: radosgw-admin zonegroup placement list
      # Retry a bit in case RGW service was just deployed
      retries: 5
      delay: 10
      register: __zonegroup_info
      tags: ["rgw"]

    - name: Get RGW zone placement info
      check_mode: false
      changed_when: false
      ansible.builtin.command:
        cmd: radosgw-admin zone placement list
      register: __zone_info
      tags: ["rgw"]

    - name: Set RGW info fact
      ansible.builtin.set_fact:
        zonegroup_info: "{{ __zonegroup_info.stdout | from_json | items2dict(key_name='key', value_name='val') }}"
        zone_info: "{{ __zone_info.stdout | from_json | items2dict(key_name='key', value_name='val') }}"
      tags: ["rgw"]

    - name: Add missing zonegroup placements  # noqa no-changed-when
      ansible.builtin.command:
        cmd: "radosgw-admin zonegroup placement add --rgw-zonegroup default --placement-id {{ item }}"
      when: item not in zonegroup_info
      loop:
        - ssd
        # - hdd
      tags: ["rgw"]

    - name: Add missing zone placements  # noqa no-changed-when
      ansible.builtin.command:
        cmd: >-
          radosgw-admin zone placement add
          --rgw-zone default
          --placement-id {{ item }}
          --data-pool default.rgw.{{ item }}.data
          --index-pool default.rgw.{{ item }}.index
          --data-extra-pool default.rgw.{{ item }}.non-ec
          --compression zstd
      when: item not in zone_info
      loop:
        - ssd
        # - hdd
      tags: ["rgw"]

    - name: Set default placement zone to ssd  # noqa no-changed-when
      ansible.builtin.command:
        cmd: radosgw-admin zonegroup placement default --rgw-zonegroup default --placement-id ssd
      when: "'default-placement' in zonegroup_info or 'default-placement' in zone_info"
      tags: ["rgw"]

    - name: Remove default-placement zonegroup placement  # noqa no-changed-when
      ansible.builtin.command:
        cmd: radosgw-admin zonegroup placement rm --placement-id default-placement
      when: "'default-placement' in zonegroup_info"
      tags: ["rgw"]

    - name: Remove default-placement zone placement  # noqa no-changed-when
      ansible.builtin.command:
        cmd: radosgw-admin zone placement rm --placement-id default-placement
      when: "'default-placement' in zone_info"
      tags: ["rgw"]

    - name: Get default pool crush rules
      check_mode: false
      changed_when: false
      ansible.builtin.command:
        cmd: "ceph osd pool get {{ item }} crush_rule -f json"
      loop:
        - default.rgw.control
        - default.rgw.log
        - default.rgw.meta
        - .rgw.root
        - .mgr
      register: __default_pool_info
      tags: ["pool", "default"]

    - name: Set fact for default pools that must be changed
      ansible.builtin.set_fact:
        default_pool_change_crush: >-
          {%- set tmp = [] -%}
          {%- for item in __default_pool_info.results -%}
          {%- if (item.stdout | from_json).crush_rule != "replicated_host_ssd" -%}
          {{- tmp.append(item.item) -}}
          {%- endif -%}
          {%- endfor -%}
          {{ tmp }}
      tags: ["pool", "default"]

    - name: Set default pool CRUSH rule to replicated_host_ssd  # noqa no-changed-when
      ansible.builtin.command:
        cmd: "ceph osd pool set {{ item }} crush_rule replicated_host_ssd"
      loop: "{{ default_pool_change_crush }}"
      tags: ["pool", "default"]

    - name: Ensure keys exist
      ceph.automation.ceph_key:
        name: "{{ item.name }}"
        state: present
        caps: "{{ item.caps }}"
      loop: "{{ ceph_keys }}"
      tags: ["keys"]

    - name: Get key info
      check_mode: false
      changed_when: false
      ansible.builtin.command:
        cmd: "ceph auth get {{ item.name }}"
      register: key_info
      loop: "{{ ceph_keys }}"
      tags: ["keys"]

    - name: Write keys to file
      run_once: false
      ansible.builtin.copy:
        content: "{{ item.stdout }}\n"
        dest: "/etc/ceph/ceph.{{ item.item.name }}.keyring"
        owner: root
        group: root
        mode: "0600"
      loop: "{{ key_info.results }}"
      loop_control:
        label: "{{ item.item.name }}"
      tags: ["keys"]

  handlers:
    - name: Create dashboard ssl cert  # noqa no-changed-when
      ansible.builtin.command:
        cmd: ceph dashboard create-self-signed-cert
      notify: Restart manager

    - name: Restart manager  # noqa no-changed-when
      ansible.builtin.command:
        cmd: ceph mgr fail mgr
      notify: Restart manager

- name: Host configuration
  hosts: ceph
  gather_facts: false
  tags: ["host-config"]
  tasks:
    - name: Mount CephFS
      ansible.posix.mount:
        src: hv@.cephfs=/
        path: /mnt/ceph
        fstype: ceph
        opts: "noatime,nofail,x-systemd.device-timeout=30"
        state: mounted
      tags: ["cephfs", "mount"]

    - name: Mount Tank
      ansible.posix.mount:
        src: hv@.tank=/
        path: /mnt/tank
        fstype: ceph
        opts: "noatime,nofail,x-systemd.device-timeout=30"
        state: mounted
      tags: ["cephfs", "mount"]

    - name: Set CephFS SELinux context
      community.general.sefcontext:
        target: "/mnt/ceph(/.*)?"
        seuser: system_u
        setype: var_t
        state: present
      notify: Restore ceph SELinux context
      when: ansible_selinux.status == "enabled"
      tags: ["cephfs", "selinux"]

    - name: Create libvirt dir on CephFS
      run_once: true
      ansible.builtin.file:
        path: /mnt/ceph/libvirt
        state: directory
        owner: root
        group: root
        mode: "0755"
      tags: ["cephfs", "libvirt"]

    - name: Set libvirt CephFS SELinux context
      community.general.sefcontext:
        target: "/mnt/ceph/libvirt(/.*)?"
        seuser: system_u
        setype: virt_image_t
        state: present
      notify: Restore libvirt SELinux context
      when: ansible_selinux.status == "enabled"
      tags: ["cephfs", "selinux"]

    - name: Set libvirt ignition CephFS SELinux context
      community.general.sefcontext:
        target: "/mnt/ceph/libvirt/.*\\.ign"
        seuser: system_u
        setype: virt_content_t
        state: present
      notify: Restore libvirt SELinux context
      when: ansible_selinux.status == "enabled"
      tags: ["cephfs", "selinux"]

    - name: Define libvirt CephFS pool
      community.libvirt.virt_pool:
        name: cephfs
        command: define
        xml: "{{ lookup('file', repo_base_dir + '/files/ceph/libvirt-cephfs.xml') }}"
        state: "present"
      tags: ["libvirt", "cephfs"]

    - name: Start libvirt CephFS pool
      community.libvirt.virt_pool:
        name: cephfs
        state: active
        autostart: true
      tags: ["libvirt", "cephfs"]

    - name: Ensure libvirt CephFS pool is autostarted
      community.libvirt.virt_pool:
        name: cephfs
        autostart: true
      tags: ["libvirt", "cephfs"]

    - name: Create my user
      ansible.builtin.user:
        name: "{{ my_user }}"
        password: "{{ my_user_pass }}"
        shell: "/usr/bin/zsh"
        home: "/mnt/ceph/{{ my_user }}"
        create_home: true
        groups: wheel
        append: true
        uid: 1000
      tags: ["user"]

    # Based on defaults from 'semanage fcontext -l | grep user_home'
    - name: Set user dir CephFS SELinux context
      community.general.sefcontext:
        target: "/mnt/ceph/{{ my_user }}"
        ftype: d
        seuser: unconfined_u
        setype: user_home_dir_t
        state: present
      notify: Restore user SELinux context
      when: ansible_selinux.status == "enabled"
      tags: ["user", "selinux"]

    - name: Set user files CephFS SELinux context
      community.general.sefcontext:
        target: "/mnt/ceph/{{ my_user }}/.+"
        seuser: unconfined_u
        setype: user_home_t
        state: present
      notify: Restore user SELinux context
      when: ansible_selinux.status == "enabled"
      tags: ["user", "selinux"]

    - name: Configure user SSH keys
      run_once: true
      ansible.posix.authorized_key:
        user: "{{ my_user }}"
        key: "{{ ssh_authorized_keys | join('\n') }}"
        state: present
        exclusive: true
      tags: ["user", "ssh"]

  handlers:
    - name: Restore ceph SELinux context  # noqa no-changed-when - handler
      run_once: true
      ansible.builtin.command: restorecon -r /mnt/ceph

    - name: Restore libvirt SELinux context  # noqa no-changed-when - handler
      run_once: true
      ansible.builtin.command: restorecon -r /mnt/ceph/libvirt

    - name: Restore user SELinux context  # noqa no-changed-when - handler
      run_once: true
      ansible.builtin.command: "restorecon -r /mnt/ceph/{{ my_user }}"

- name: Configure other stuff
  hosts: ceph
  tasks:
    - name: Allow keepalived connections
      ansible.posix.seboolean:
        name: nis_enabled
        state: true
        persistent: true
      when:
        - ansible_os_family == 'RedHat'
        - ansible_selinux.status == "enabled"
      tags: ["keepalived"]

    - name: Disable keepalived
      ansible.builtin.systemd:
        name: keepalived.service
        state: stopped
        enabled: false
      register: __svc
      failed_when:
        - __svc.failed
        - "'Could not find the requested service' not in __svc.msg"
      when: inventory_hostname not in keepalived_hosts
      tags: ["keepalived"]

    - name: Remove keepalived config
      ansible.builtin.file:
        path: "/etc/keepalived/keepalived.conf"
        state: absent
      when: inventory_hostname not in keepalived_hosts
      tags: ["keepalived"]

    # Use include to avoid crash due to invalid keepalived_instances on some hosts
    - name: Include keepalived role
      ansible.builtin.include_role:
        name: keepalived
        apply:
          tags: ["keepalived"]
      when: inventory_hostname in keepalived_hosts
      tags: ["keepalived"]
  roles:
    - role: samba
      tags: ["samba"]
