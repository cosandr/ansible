#!/usr/bin/env -S ansible-playbook
---

- name: Ceph host preparation
  hosts: ceph
  gather_facts: true
  gather_subset: ["min"]
  tags: ["system"]
  tasks:
    - name: Download Ceph GPG key
      ansible.builtin.get_url:
        url: https://eu.ceph.com/keys/release.gpg
        dest: /etc/apt/keyrings/ceph-keyring.gpg
        owner: root
        group: root
        mode: "0644"
      diff: false
      tags: ["apt"]

    - name: Add Ceph repo
      ansible.builtin.apt_repository:
        repo: "deb [arch=amd64 signed-by=/etc/apt/keyrings/ceph-keyring.gpg] https://eu.ceph.com/debian-{{ ceph_version }} {{ ansible_distribution_release | lower }} main"
        state: present
        filename: ceph
      tags: ["apt"]

    - name: Prefer repo packages
      ansible.builtin.copy:
        content: |
          Package: *
          Pin: origin ceph.com
          Pin-Priority: 900
        dest: "/etc/apt/preferences.d/99ceph"
        owner: root
        group: root
        mode: "0644"
      tags: ["apt"]

    # Might not work, dpkg-divert should work if this doesn't
    - name: Prevent ceph-common logrotate file from being installed
      ansible.builtin.copy:
        content: |
          path-exclude /etc/logrotate.d/ceph-common
        dest: /etc/dpkg/dpkg.cfg.d/99-no-ceph-common
        owner: root
        group: root
        mode: "0644"
      tags: ["apt"]

    - name: Install Ceph packages
      ansible.builtin.apt:
        name:
          - cephadm
          - ceph-common
        state: present
      tags: ["install"]

- name: Ceph configuration
  hosts: ceph
  gather_facts: false
  tags: ["configure"]
  tasks:
    - name: Run on only one host
      run_once: true
      tags: ["orch"]
      block:
        - name: Deploy RGW service
          ceph_orch_apply:
            spec: "{{ lookup('template', repo_base_dir + '/files/ceph/rgw-main.yml.j2') }}"
          tags: ["rgw"]

    - name: Mount CephFS
      ansible.posix.mount:
        src: admin@.cephfs=/
        path: /mnt/ceph
        fstype: ceph
        opts: "noatime,nofail,x-systemd.device-timeout=30"
        state: mounted
      tags: ["cephfs"]

    - name: Set CephFS SELinux context
      community.general.sefcontext:
        target: "/mnt/ceph(/.*)?"
        seuser: system_u
        setype: var_t
        state: present
      notify: Restore ceph SELinux context
      when: ansible_selinux.status == "enabled"
      tags: ["cephfs", "selinux"]

    - name: Create libvirt dir on CephFS
      run_once: true
      ansible.builtin.file:
        path: /mnt/ceph/libvirt
        state: directory
        owner: root
        group: root
        mode: "0755"
      tags: ["cephfs", "libvirt"]

    - name: Set libvirt CephFS SELinux context
      community.general.sefcontext:
        target: "/mnt/ceph/libvirt(/.*)?"
        seuser: system_u
        setype: virt_image_t
        state: present
      notify: Restore libvirt SELinux context
      when: ansible_selinux.status == "enabled"
      tags: ["cephfs", "selinux"]

    - name: Set libvirt ignition CephFS SELinux context
      community.general.sefcontext:
        target: "/mnt/ceph/libvirt/.*\\.ign"
        seuser: system_u
        setype: virt_content_t
        state: present
      notify: Restore libvirt SELinux context
      when: ansible_selinux.status == "enabled"
      tags: ["cephfs", "selinux"]

    - name: Define libvirt CephFS pool
      community.libvirt.virt_pool:
        name: cephfs
        command: define
        xml: "{{ lookup('file', repo_base_dir + '/files/ceph/libvirt-cephfs.xml') }}"
        state: "present"
      tags: ["libvirt", "cephfs"]

    - name: Start libvirt CephFS pool
      community.libvirt.virt_pool:
        name: cephfs
        state: active
        autostart: true
      tags: ["libvirt", "cephfs"]

    - name: Ensure libvirt CephFS pool is autostarted
      community.libvirt.virt_pool:
        name: cephfs
        autostart: true
      tags: ["libvirt", "cephfs"]

    - name: Create my user
      ansible.builtin.user:
        name: "{{ my_user }}"
        password: "{{ my_user_pass }}"
        shell: "/usr/bin/zsh"
        home: "/mnt/ceph/{{ my_user }}"
        create_home: true
        groups: wheel
        append: true
        uid: 1000
      tags: ["user"]

    # Based on defaults from 'semanage fcontext -l | grep user_home'
    - name: Set user dir CephFS SELinux context
      community.general.sefcontext:
        target: "/mnt/ceph/{{ my_user }}"
        ftype: d
        seuser: unconfined_u
        setype: user_home_dir_t
        state: present
      notify: Restore user SELinux context
      when: ansible_selinux.status == "enabled"
      tags: ["user", "selinux"]

    - name: Set user files CephFS SELinux context
      community.general.sefcontext:
        target: "/mnt/ceph/{{ my_user }}/.+"
        seuser: unconfined_u
        setype: user_home_t
        state: present
      notify: Restore user SELinux context
      when: ansible_selinux.status == "enabled"
      tags: ["user", "selinux"]

    - name: Configure user SSH keys
      run_once: true
      ansible.posix.authorized_key:
        user: "{{ my_user }}"
        key: "{{ ssh_authorized_keys | join('\n') }}"
        state: present
        exclusive: true
      tags: ["user", "ssh"]

  roles:
    - role: ceph_nfs
      run_once: true
      tags: ["nfs"]

  handlers:
    - name: Restore ceph SELinux context  # noqa no-changed-when - handler
      run_once: true
      ansible.builtin.command: restorecon -r /mnt/ceph

    - name: Restore libvirt SELinux context  # noqa no-changed-when - handler
      run_once: true
      ansible.builtin.command: restorecon -r /mnt/ceph/libvirt

    - name: Restore user SELinux context  # noqa no-changed-when - handler
      run_once: true
      ansible.builtin.command: "restorecon -r /mnt/ceph/{{ my_user }}"

- name: Configure other stuff
  hosts: ceph
  tasks:
    - name: Allow keepalived connections
      ansible.posix.seboolean:
        name: nis_enabled
        state: true
        persistent: true
      when:
        - ansible_os_family == 'RedHat'
        - ansible_selinux.status == "enabled"
      tags: ["keepalived"]

    # Use include to avoid crash due to invalid keepalived_instances on some hosts
    - name: Include keepalived role
      ansible.builtin.include_role:
        name: keepalived
        apply:
          tags: ["keepalived"]
      when: inventory_hostname in keepalived_hosts
      tags: ["keepalived"]
