---

ceph_alertmanager_api_host: "http://prom01.{{ hostvars['prom01'].domain }}:{{ hostvars['prom01'].alertmanager_port }}"
ceph_prometheus_api_host: "http://prom01.{{ hostvars['prom01'].domain }}:{{ hostvars['prom01'].prometheus_port }}"
ceph_config:
  - option: "mgr/cephadm/autotune_memory_target_ratio"
    value: "0.2"
    who: mgr
  - option: "mgr/cephadm/device_enhanced_scan"
    value: "true"
    who: mgr
  - option: "mgr/dashboard/standby_behaviour"
    value: "error"
    who: mgr
  - option: "mgr/dashboard/standby_error_status_code"
    value: "503"
    who: mgr
  - option: "osd_memory_target_autotune"
    value: "true"
    who: osd
  - option: log_to_file
    value: "true"
  - option: log_to_journald
    value: "false"
  - option: log_to_stderr
    value: "false"
  - option: mon_cluster_log_to_file
    value: "true"
  - option: mon_cluster_log_to_journald
    value: "false"
  - option: mon_cluster_log_to_stderr
    value: "false"
  - option: osd_pool_default_pg_num
    value: 16
  - option: osd_pool_default_size
    value: 3
  - option: osd_pool_default_min_size
    value: 2
  - option: mon_max_pg_per_osd
    value: 350
  - option: bluestore_compression_algorithm
    value: lz4
  - option: bluestore_compression_mode
    value: aggressive
  - option: rgw_trust_forwarded_https
    value: "true"
  - option: mon_data_avail_warn
    value: 20
    who: mon

ceph_pools:
  # CephFS
  - name: cephfs.meta
    application: cephfs
    pool_type: replicated
    rule_name: replicated_host_ssd
    zstd_compression: true
  - name: cephfs.data
    application: cephfs
    pool_type: erasure
    erasure_profile: ec_host_ssd32
    zstd_compression: true
    ec_overwrites: true

  # RGW
  - name: default.rgw.buckets.index
    application: rgw
    pool_type: replicated
    rule_name: replicated_host_ssd
  - name: default.rgw.buckets.non-ec
    application: rgw
    pool_type: replicated
    rule_name: replicated_host_ssd
  - name: default.rgw.buckets.data
    application: rgw
    pool_type: erasure
    erasure_profile: ec_host_ssd32
    zstd_compression: true
    ec_overwrites: true

  # RBD
  - name: rbd.libvirt
    application: rbd
    pool_type: replicated
    rule_name: replicated_host_ssd
    zstd_compression: true

  - name: rbd.libvirt.data
    application: rbd
    pool_type: erasure
    erasure_profile: ec_host_ssd32
    zstd_compression: true
    ec_overwrites: true

  - name: rbd.talos
    application: rbd
    pool_type: replicated
    rule_name: replicated_host_ssd
    zstd_compression: true

  - name: rbd.talos.data
    application: rbd
    pool_type: erasure
    erasure_profile: ec_host_ssd32
    zstd_compression: true
    ec_overwrites: true

ceph_keys:
  - name: client.hv
    caps:
      mds: "allow rws fsname=cephfs"
      mon: "allow r fsname=cephfs"
      osd: "allow rw tag cephfs data=cephfs"
  - name: client.libvirt
    caps:
      mon: "profile rbd"
      osd: "profile rbd pool=rbd.libvirt, profile rbd pool=rbd.libvirt.data"
  # https://github.com/ceph/ceph-csi/blob/devel/docs/capabilities.md#rbd
  - name: client.talos_rbd
    secret: "{{ vault_ceph_keys.talos_rbd }}"
    no_write_host: true
    caps:
      mon: "profile rbd"
      osd: "profile rbd pool=rbd.talos, profile rbd pool=rbd.talos.data"
      mgr: "profile rbd pool=rbd.talos, profile rbd pool=rbd.talos.data"

cephfs_snap_schedules:
  - fs: cephfs
    path: /
    retention:
      days: 2
      weeks: 1
