---

- name: Set kube_name fact
  ansible.builtin.set_fact:
    kube_name: "{{ ('talos' in group_names) | ternary('talos', 'kube') }}"
  tags: ["always"]

- name: Install Helm diff plugin
  delegate_to: localhost
  kubernetes.core.helm_plugin:
    plugin_path: https://github.com/databus23/helm-diff
    state: present
  tags: ["helm", "plugins"]

- name: Create Cilium monitoring pool
  kubernetes.core.k8s:
    state: present
    resource_definition:
      apiVersion: cilium.io/v2alpha1
      kind: CiliumPodIPPool
      metadata:
        name: monitoring
      spec:
        ipv4:
          cidrs: "{{ subnets[kube_name].mon | ansible.utils.ipv4 }}"
          # Add 2 to CIDR, i.e. /26 -> /28
          # Needs to be increased if there's more than 4 nodes in cluster.
          # maskSize: "{{ subnets[kube_name].mon | first | ansible.utils.ipaddr('prefix') + 2 }}"
          # spec.ipv4.maskSize in body must be of type integer
          maskSize: 28
  when: cilium_version is defined
  tags: ["cilium", "pool"]

- name: Create Calico monitoring pool
  kubernetes.core.k8s:
    state: present
    resource_definition:
      apiVersion: projectcalico.org/v3
      kind: IPPool
      metadata:
        name: monitoring
      spec:
        cidr: "{{ subnets[kube_name].mon | ansible.utils.ipv4 | first }}"
        allowedUses: ["Workload"]
  when: cilium_version is not defined
  tags: ["calico", "pool"]

- name: Add helm repos
  kubernetes.core.helm_repository:
    name: "{{ item.name }}"
    repo_url: "{{ item.repo_url }}"
  loop:
    - name: prometheus-community
      repo_url: "https://prometheus-community.github.io/helm-charts"
    - name: ingress-nginx
      repo_url: "https://kubernetes.github.io/ingress-nginx"
    - name: grafana
      repo_url: "https://grafana.github.io/helm-charts"
    - name: jetstack
      repo_url: "https://charts.jetstack.io"
    - name: cert-manager-webhook-hetzner
      repo_url: "https://vadimkim.github.io/cert-manager-webhook-hetzner"
    - name: openebs-lvmlocalpv
      repo_url: "https://openebs.github.io/lvm-localpv"
  loop_control:
    label: "{{ item.name }}"
  tags: ["helm", "repos"]

- name: Create OpenEBS namespace
  kubernetes.core.k8s:
    state: present
    resource_definition:
      apiVersion: v1
      kind: Namespace
      metadata:
        name: openebs
        labels: "{{ pss_privileged_labels }}"
  tags: ["lvm"]

- name: Install OpenEBS LVM CSI driver
  kubernetes.core.helm:
    name: lvm-localpv
    chart_ref: openebs-lvmlocalpv/lvm-localpv
    chart_version: "{{ openebs_lvm_version }}"
    release_namespace: openebs
    create_namespace: false
    update_repo_cache: true
    wait: true
    release_values:
      lvmController:
        tolerations:
          - key: "node-role.kubernetes.io/control-plane"
            effect: "NoSchedule"
            operator: "Exists"
      lvmNode:
        tolerations:
          - key: "node-role.kubernetes.io/control-plane"
            effect: "NoSchedule"
            operator: "Exists"
  tags: ["lvm"]

- name: Create LVM StorageClass
  kubernetes.core.k8s:
    state: present
    resource_definition:
      apiVersion: storage.k8s.io/v1
      kind: StorageClass
      metadata:
        name: openebs-lvmpv
      allowVolumeExpansion: true
      # Fixes PV getting created on unschedulable node
      volumeBindingMode: WaitForFirstConsumer
      provisioner: local.csi.openebs.io
      parameters:
        storage: "lvm"
        volgroup: "localpv"
        fsType: "ext4"
      mountOptions:
        - "defaults"
        - "noatime"
  tags: ["lvm"]

- name: Create monitoring namespace
  kubernetes.core.k8s:
    state: present
    resource_definition:
      apiVersion: v1
      kind: Namespace
      metadata:
        name: monitoring
        labels: "{{ pss_privileged_labels }}"
        annotations: "{{ __cilium_pool if cilium_version is defined else __calico_pool }}"
  vars:
    __calico_pool:
      cni.projectcalico.org/ipv4pools: '["monitoring"]'
    __cilium_pool:
      ipam.cilium.io/ip-pool: monitoring
  tags: ["helm", "prometheus"]

# https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack
- name: Deploy Prometheus chart
  kubernetes.core.helm:
    name: prometheus
    chart_ref: prometheus-community/kube-prometheus-stack
    chart_version: "{{ prometheus_version }}"
    release_namespace: monitoring
    create_namespace: false
    update_repo_cache: true
    release_values: "{{ lookup('template', '{{ repo_base_dir }}/files/k8s/prom.yml.j2') | from_yaml }}"
  tags: ["helm", "prometheus"]

# https://github.com/kubernetes/ingress-nginx/tree/main/charts/ingress-nginx
- name: Deploy ingress-nginx chart
  kubernetes.core.helm:
    name: ingress-nginx
    chart_ref: ingress-nginx/ingress-nginx
    chart_version: "{{ ingress_nginx_version }}"
    release_namespace: ingress-nginx
    create_namespace: true
    update_repo_cache: true
    release_values: "{{ lookup('template', '{{ repo_base_dir }}/files/k8s/ingress-nginx.yml.j2') | from_yaml }}"
  tags: ["helm", "nginx"]

# https://github.com/grafana/helm-charts/tree/main/charts/promtail
- name: Deploy Promtail chart
  kubernetes.core.helm:
    name: promtail
    chart_ref: grafana/promtail
    chart_version: "{{ promtail_version }}"
    release_namespace: monitoring
    create_namespace: false
    update_repo_cache: true
    release_values: "{{ lookup('template', '{{ repo_base_dir }}/files/k8s/promtail.yml.j2') | from_yaml }}"
  tags: ["helm", "promtail"]

- name: Cert-manager tasks
  tags: ["cert-manager"]
  block:
    - name: Deploy cert-manager chart
      kubernetes.core.helm:
        name: cert-manager
        chart_ref: jetstack/cert-manager
        chart_version: "{{ certmanager_version }}"
        release_namespace: cert-manager
        create_namespace: true
        update_repo_cache: true
        release_values:
          installCRDs: true
      tags: ["helm"]

    - name: Create TSIG secret
      kubernetes.core.k8s:
        state: present
        resource_definition:
          apiVersion: v1
          kind: Secret
          metadata:
            name: tsig-secret
            namespace: cert-manager
          type: Opaque
          data:
            hb-key: "{{ rfc2136_keys['hb'] | b64encode }}"
      tags: ["secret"]

    - name: Create cluster issuers
      kubernetes.core.k8s:
        state: present
        resource_definition: "{{ lookup('template', '{{ repo_base_dir }}/files/k8s/cluster-issuer.yml.j2') | from_yaml }}"
      loop: "{{ certmanager_cluster_issuers }}"
      tags: ["issuer"]

# https://intel.github.io/intel-device-plugins-for-kubernetes/INSTALL.html
- name: Intel GPU tasks
  tags: ["gpu"]
  when: intel_gpu_operator_installed
  block:
    - name: Add NFD Helm repo
      kubernetes.core.helm_repository:
        name: nfd
        repo_url: "https://kubernetes-sigs.github.io/node-feature-discovery/charts"

    - name: Create node-feature-discovery namespace
      kubernetes.core.k8s:
        state: present
        resource_definition:
          apiVersion: v1
          kind: Namespace
          metadata:
            name: node-feature-discovery
            labels: "{{ pss_privileged_labels }}"
      tags: ["nfd"]

    - name: Install NFD
      kubernetes.core.helm:
        name: nfd
        chart_ref: nfd/node-feature-discovery
        chart_version: "{{ nfd_version }}"
        release_namespace: node-feature-discovery
        update_repo_cache: true
        release_values:
          master:
            extraLabelNs:
              - gpu.intel.com
            resourceLabels:
              - gpu.intel.com/millicores
              - gpu.intel.com/memory.max
              - gpu.intel.com/tiles
      tags: ["nfd"]

    # Can't use operator until it allows tolerations
    # https://github.com/intel/intel-device-plugins-for-kubernetes/issues/1617
    - name: Create inteldeviceplugins-system namespace
      kubernetes.core.k8s:
        state: present
        resource_definition:
          apiVersion: v1
          kind: Namespace
          metadata:
            name: inteldeviceplugins-system
            labels: "{{ pss_privileged_labels }}"
      tags: ["gpu-plugin"]

    - name: Get latest release
      ansible.builtin.uri:
        url: "https://api.github.com/repos/intel/intel-device-plugins-for-kubernetes/releases/latest"
        method: GET
        return_content: true
        status_code: 200
        body_format: json
        validate_certs: true
      check_mode: false
      register: _latest_release
      tags: ["gpu-plugin"]

    - name: Clone repo
      ansible.builtin.git:
        repo: https://github.com/intel/intel-device-plugins-for-kubernetes.git
        version: "{{ _latest_release.json.tag_name }}"
        dest: "/tmp/intel-device-plugins-for-kubernetes"
        depth: 1
      tags: ["gpu-plugin"]

    - name: Install NodeFeatureRules
      kubernetes.core.k8s:
        state: present
        resource_definition: "{{ lookup('kubernetes.core.kustomize', dir='/tmp/intel-device-plugins-for-kubernetes/deployments/nfd/overlays/node-feature-rules') }}"
      tags: ["nfd", "gpu-plugin"]

    - name: Install Intel GPU plugin
      kubernetes.core.k8s:
        state: present
        namespace: inteldeviceplugins-system
        resource_definition: "{{ lookup('kubernetes.core.kustomize', dir='/tmp/intel-device-plugins-for-kubernetes/deployments/gpu_plugin/overlays/nfd_labeled_nodes') }}"
      tags: ["gpu-plugin"]

    - name: Patch daemonset to tolerate control plane
      kubernetes.core.k8s:
        api_version: apps/v1
        kind: DaemonSet
        name: intel-gpu-plugin
        namespace: inteldeviceplugins-system
        state: patched
        resource_definition:
          spec:
            template:
              spec:
                tolerations:
                  - operator: "Exists"
                    key: node-role.kubernetes.io/control-plane
                    effect: NoSchedule
      tags: ["gpu-plugin"]
